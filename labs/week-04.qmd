---
title: "Lab Week 4. Multi-armed Bandit (A/B Testing)"
mermaid-format: png
---

## Introduction

This week's lab explores the multi-armed bandit problem—a classic framework for balancing exploration and exploitation in sequential decision-making. You'll learn to formulate A/B tests using Thompson sampling with Bayesian inference.

> You are tasked with conducting a sequential A/B test of two interface designs ($i \in \{A, B\}$). Users perform a task using one of the interfaces, and we observe an outcome of either $x \in \{\text{Success}, \text{Failure}\}$.

For each interface design, we model the probability of a successful interaction $p_i$ using:

$$
\begin{aligned}
p_i &\sim \text{Beta}(1, 1) \\
x_i &\sim \text{Bernoulli}(p_i)
\end{aligned}
$$

## Q1. Deriving Posterior Distributions

Given the following observations, derive the posterior distributions of $p_A$ and $p_B$ for design A and design B.

- Design A: [Success, Failure, Success, Success, Failure]
- Design B: [Success, Success, Success, Failure, Success]

::: {.callout-tip title="Answer" icon=false}
Given the probabilistic model

$$
\begin{aligned}
p_i &\sim \beta(1, 1) \\
x_i &\sim \text{Bernoulli}(p_i)
\end{aligned}
$$

By conjugacy of the Beta-Bernoulli distribution, the posterior distribution is given by

$$
p_i\,|\,\text{data} \sim \beta\left(\alpha_0 + \sum x_i, \beta_0 + N - \sum x_i\right)
$$

where $\alpha_0 = 1$ and $\beta_0 = 1$ are the prior parameters.

For design A, we observe 3 Successes and 2 Failures, thus

$$
p_A\,|\,\text{data} \sim \beta(1 + 3,\,1 + 2) = \beta(4, 3).
$$

And for design B, we observe 4 Successes and 1 Failure, therefore

$$
p_B\,|\,\text{data} \sim \beta(1 + 4,\,1 + 1) = \beta(5, 2).
$$
:::


## Q2. Selecting the Next Design

Based on the posterior distributions you derived, which design would the agent more likely select for the next user? Explain your reasoning.

::: {.callout-tip title="Answer" icon=false}
We recall the Thompson Sampling algorithm in a 2-armed bandit problem as follows:

```{.pseudocode}
#| label: "Thompson Sampling"
#| html-indent-size: "1.2em"
#| html-comment-delimiter: "//"
#| html-line-number: true
#| html-line-number-punc: ":"
#| html-no-end: false
#| pdf-line-number: true

\begin{algorithm}
\caption{Thompson Sampling}
\begin{algorithmic}
\State Initialize $\alpha_A = 1, \beta_A = 1$  \Comment{Prior for Design A}
\State Initialize $\alpha_B = 1, \beta_B = 1$  \Comment{Prior for Design B}
\For{each user $t = 1, 2, \dots, T$}
    \State Sample $\tilde{p}_A \sim \text{Beta}(\alpha_A, \beta_A)$
    \State Sample $\tilde{p}_B \sim \text{Beta}(\alpha_B, \beta_B)$
    \If{$\tilde{p}_A > \tilde{p}_B$}
        \State Select design $A$ for the current user
        \State Observe outcome $x \in \{\text{Success}, \text{Failure}\}$
        \State Update $\alpha_A \leftarrow \alpha_A + x$, $\beta_A \leftarrow \beta_A + (1-x)$
    \Else
        \State Select design $B$ for the current user
        \State Observe outcome $x \in \{\text{Success}, \text{Failure}\}$
        \State Update $\alpha_B \leftarrow \alpha_B + x$, $\beta_B \leftarrow \beta_B + (1-x)$
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}
```

The agent would recommend design A if and only if the sampled values $\tilde{p}_{A} > \tilde{p}_{B}$. We have the posterior distributions

$$
p_A \sim \text{Beta}(4, 3) \text{; and} \quad p_B \sim \text{Beta}(5, 2).
$$

It is obvious that $p_B$ will likely draw a larger sample, hence the agent is more likely to select design B.

:::

## Q3. Balancing Exploration and Exploitation

Suppose you need to conduct this A/B test with 1000 users in a sequential manner. Explain how Thompson sampling (or bandit algorithms in general) helps balance exploration and exploitation in this context.

::: {.callout-tip title="Answer" icon=false}
Thompson sampling exploits the properties of the Beta distribution to balance between exploration and exploitation.

Given an initally weakly informative prior (i.e. $\text{Beta}(1,\,1)$) for each bandits, these distributions are initialized with maximal kurtosis/uncertainty. At this stage, the algorithm is equivalent to sampling from arms with a discrete uniform probability distribution.

As samples accumulate and the posteriors get updated with $\text{Beta}(1 + \text{number of successes}, 1 + \text{number of failures})$, the distributions are less kurtotic. The arms with higher empirical rate of success will be drawn with overwhelmingly higher probabilities.
:::

## Q4. Thompson Sampling Implementation

Implement a Thompson sampling algorithm to simulate the sequential A/B test described above using NumPyro.

Requirements:

1. Initialize Beta priors for both designs A and B as Beta(1, 1)
2. For each of 1000 users:
   - Sample from the posterior distributions of $p_A$ and $p_B$
   - Select the design with the higher sampled value
   - Simulate an outcome (Success/Failure) based on the true success probabilities: $p_A = 0.4$ and $p_B = 0.6$
   - Update the posterior distribution for tgithe selected design

After all 1000 users, report:

- The total number of times each design was selected
- The total number of successes for each design
- The final posterior distributions (alpha and beta parameters) for both designs

::: {.callout-tip title="Answer" icon=false}

```{python}
#| eval: true
#| output: true

import numpy as np
import numpyro
import numpyro.distributions as dist
from jax import random
import jax.numpy as jnp

# Set random seed for reproducibility
np.random.seed(42)

# True success probabilities (ground truth)
TRUE_P_A = 0.4
TRUE_P_B = 0.6

# Initialize prior parameters (Beta(1, 1) = Uniform prior)
alpha_A, beta_A = 1, 1  # Prior for Design A
alpha_B, beta_B = 1, 1  # Prior for Design B

# Track selections and outcomes
selections_A = 0
selections_B = 0
successes_A = 0
successes_B = 0

# Number of users
n_users = 1000

print(f"Starting Thompson Sampling with {n_users} users")
print(f"True success rates: A={TRUE_P_A}, B={TRUE_P_B}")
print("-" * 50)

# Thompson Sampling Algorithm
for user in range(n_users):
    # Step 1: Sample from posterior distributions
    # Posterior is Beta(alpha, beta) given observed data
    sample_A = np.random.beta(alpha_A, beta_A)
    sample_B = np.random.beta(alpha_B, beta_B)

    # Step 2: Select the design with higher sampled value
    if sample_A > sample_B:
        selected_design = 'A'
        selections_A += 1
        # Simulate outcome based on TRUE_P_A (ground truth)
        outcome = np.random.rand() < TRUE_P_A
    else:
        selected_design = 'B'
        selections_B += 1
        # Simulate outcome based on TRUE_P_B (ground truth)
        outcome = np.random.rand() < TRUE_P_B

    # Step 3: Update posterior parameters
    if selected_design == 'A':
        alpha_A += int(outcome)  # Add 1 if success, 0 if failure
        beta_A += 1 - int(outcome)  # Add 1 if failure, 0 if success
        successes_A += int(outcome)
    else:
        alpha_B += int(outcome)
        beta_B += 1 - int(outcome)
        successes_B += int(outcome)

# Print results
print("\n" + "=" * 50)
print("THOMPSON SAMPLING RESULTS")
print("=" * 50)
print(f"\nDesign A:")
print(f"  Times selected: {selections_A} ({100*selections_A/n_users:.1f}%)")
print(f"  Total successes: {successes_A}")
print(f"  Empirical success rate: {successes_A/selections_A:.3f}" if selections_A > 0 else "  N/A")
print(f"  Final posterior: Beta({alpha_A}, {beta_A})")

print(f"\nDesign B:")
print(f"  Times selected: {selections_B} ({100*selections_B/n_users:.1f}%)")
print(f"  Total successes: {successes_B}")
print(f"  Empirical success rate: {successes_B/selections_B:.3f}" if selections_B > 0 else "  N/A")
print(f"  Final posterior: Beta({alpha_B}, {beta_B})")

print(f"\nTotal successes: {successes_A + successes_B}")
print(f"Overall success rate: {(successes_A + successes_B)/n_users:.3f}")
```

:::

## Deliverable

Submit a zip file to eLearn containing:

1. Written answers to Q1–Q3 with explanations and calculations
2. NumPyro code for Q4 implementing Thompson sampling algorithm
