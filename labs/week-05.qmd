---
title: "Lab Week 5. Markov Decision Process (MDP)"
mermaid-format: png
---

## Introduction

Read the scenario below and complete Q1–Q4. Each question builds on the previous one, progressing from model specification to policy analysis to implementation.

A smart home conversational agent helps users adjust their environment by gathering preferences. The agent must collect temperature preference (warm/cool) and lighting preference (bright/dim) before making adjustments. Users may respond clearly, give unclear responses, or abandon the interaction if frustrated. The agent must decide: should it ask questions one at a time or both at once?

## Q1. MDP Formulation

Model the smart home agent as an MDP by specifying:

- **State space**: Define the initial state, interim states (where the agent has partial information), and terminal states.

- **Action space**: For each non-terminal state, list the available actions.

- **Transition probabilities**: For state $s_0$ (initial state with no information) and action `ask_temperature`, specify the probability distribution over next states. Consider realistic cases where users might provide clear answers, unclear responses, or abandon.

- **Reward function**: Define the reward $R(s, a, s')$ for reaching terminal states.

Explain your design choices.

::: {.callout-tip title="Answer" icon=false}
### State Space

The state space consists of discrete states representing what information has been collected:

- **$S_0$**: Initial state - no preferences collected
- **$T$**: Temperature preference collected
- **$L$**: Lighting preference collected
- **$TL$**: Both preferences collected (ready to actuate)
- **SUCCESS**: Terminal state - successful interaction
- **FAILURE**: Terminal state - user abandoned

### Action Space

For each non-terminal state, available actions are:

| State | Available Actions |
|-------|-------------------|
| $S_0$ | ASK_TEMPERATURE, ASK_LIGHTING, ASK_BOTH |
| $T$ | ASK_LIGHTING |
| $L$ | ASK_TEMPERATURE |
| $TL$ | ACTUATE |

### Transition Probabilities

From the initial state $S_0$:

| Origin | Action | Destination | Probability Mass |
|--------|--------|-------------|------------------|
| $S_0$ | ASK_TEMPERATURE | $T$ | 0.7 |
| $S_0$ | ASK_TEMPERATURE | $S_0$ | 0.2 |
| $S_0$ | ASK_TEMPERATURE | FAILURE | 0.1 |
| $S_0$ | ASK_LIGHTING | $L$ | 0.7 |
| $S_0$ | ASK_LIGHTING | $S_0$ | 0.2 |
| $S_0$ | ASK_LIGHTING | FAILURE | 0.1 |
| $S_0$ | ASK_BOTH | $TL$ | 0.49 |
| $S_0$ | ASK_BOTH | $T$ | 0.21 |
| $S_0$ | ASK_BOTH | $L$ | 0.21 |
| $S_0$ | ASK_BOTH | $S_0$ | 0.04 |
| $S_0$ | ASK_BOTH | FAILURE | 0.05 |

From states $T$, $L$:

| Origin | Action | Destination | Probability Mass |
|--------|--------|-------------|------------------|
| $T$ | ASK_LIGHTING | $TL$ | 0.7 |
| $T$ | ASK_LIGHTING | $T$ | 0.2 |
| $T$ | ASK_LIGHTING | FAILURE | 0.1 |
| $L$ | ASK_TEMPERATURE | $TL$ | 0.7 |
| $L$ | ASK_TEMPERATURE | $L$ | 0.2 |
| $L$ | ASK_TEMPERATURE | FAILURE | 0.1 |

From state $TL$:

| Origin | Action | Destination | Probability Mass |
|--------|--------|-------------|------------------|
| $TL$ | ACTUATE | SUCCESS | 0.9 |
| $TL$ | ACTUATE | FAILURE | 0.1 |

### Reward Function

Using a discounting model with discount factor $\gamma = 0.95$:

| Transition | Reward |
|------------|--------|
| $\to$ SUCCESS | $10$ |
| $\to$ FAILURE | $0$ |

where $t$ is the current time step (0-indexed).

### Design Choices

**Discrete State Space**: The state space tracks only what information has been collected (temperature, lighting, both, or neither). This abstraction models user frustration with transition probability masses.

**Transition Probabilities**: Each question has three possible outcomes:
- Clear response (70%): Successfully collect the preference
- Unclear response (20%): User responds but incompletely; agent retries from the same state
- Abandon (10%): User gives up; terminal `FAILURE` state

**ASK_BOTH Compound Probabilities**: When asking both questions simultaneously, outcomes are independent. The probability of getting both clear answers is $0.7 \times 0.7 = 0.49$. Partial successes (one clear, one unclear) transition to states $T$ or $L$, allowing the agent to recover.
:::

## Q2. State Transition Diagram

Draw a state transition diagram for your MDP showing:

- All states (use circles or boxes)

- Possible transitions between states for at least one action from the initial state

- Transition probabilities on the edges

Your diagram should help visualize how the agent and user move through the interaction.

::: {.callout-tip title="Answer" icon=false}

The state transition diagram below visualizes the MDP structure. States are shown as nodes, with transitions labeled by actions and probabilities. Terminal states (SUCCESS, FAILURE) are highlighted.

::: {.content-visible when-format="html"}
![](figures/mdp-state-diagram.svg){fig-align="center" width="80%"}

:::

::: {.content-visible when-format="pdf"}
![](figures/mdp-state-diagram.png){fig-align="center" width="80%"}

:::

**Legend:**

- **$S_0$**: Initial state with no preferences collected
- **$T$**: Temperature preference collected
- **$L$**: Lighting preference collected
- **$TL$**: Both preferences collected (ready to actuate)
- **✓ SUCCESS**: Terminal - user satisfied with adjustment
- **✗ FAILURE**: Terminal - user abandoned interaction

:::

## Q3. Policy Analysis

Consider the following Python code that implements two policies for the smart home agent:

```python
def policy_sequential(state: State) -> Action:
    """Ask questions one at a time: temperature first, then lighting."""
    if state == State.S0:
        return Action.ASK_TEMPERATURE
    elif state == State.T:
        return Action.ASK_LIGHTING
    elif state == State.L:
        return Action.ASK_TEMPERATURE
    elif state == State.TL:
        return Action.ADJUST
    else:
        raise ValueError(f"No action for state {state}")


def policy_simultaneous(state: State) -> Action:
    """Ask both questions together when possible."""
    if state == State.S0:
        return Action.ASK_BOTH
    elif state == State.T:
        return Action.ASK_LIGHTING
    elif state == State.L:
        return Action.ASK_TEMPERATURE
    elif state == State.TL:
        return Action.ADJUST
    else:
        raise ValueError(f"No action for state {state}")
```

Explain the key difference between these two policies. Which policy would likely achieve a shorter average interaction length? Why? Under what conditions (consider transition probabilities) might `policy_simultaneous` have a lower success rate than `policy_sequential`?

::: {.callout-tip title="Answer" icon="false"}
`policy_sequential` always execute `ASK_TEMPERATURE` and will not transition to `ASK_LIGHTING` until the temperature preference has been ascertained, whereas `policy_simultaneous` always execute `ASK_BOTH` from $S_0$ and only execute `ASK_LIGHITNG` and `ASK_TEMPERATURE` when the complement information is present.

Intuitively, `policy_simultaneous` should on average result in a shorter execution cycle and a higher success rate. In reality, this would depend on the exact transition probabilities. Given the assumed probabilities specified in the answers to question 1, this can be computed explicitly as follows.

### Number of Interactions before Success

#### For `policy_sequential`

First we compute the expected number of actions to go from $S_0$ to $T$. Conditioning on eventual success, the number of transitions to state $T$ follows a geometric distribution - $X_T \sim \text{Geometric}\left(\displaystyle\frac{7}{9}\right)$, and thus $\mathbb{E}(X_T) = \displaystyle\frac{9}{7} \approx 1.2857$.

And similarly, the expected number of actions to go from $T$ to $TL$, conditioning on eventual success, is also $\displaystyle\frac{9}{7} \approx 1.2857$.

Adding a final actuation step, the expected total number of steps to success is $\displaystyle\frac{9}{7} + \displaystyle\frac{9}{7} + 1 \approx 3.5714$

To compute the probability of success, again we first look at the transition from $S_0$ to $T$. We observe that the end state is binary - $T$ or $\text{FAILURE}$. We let $p$ be the eventual probability of failure and solve a recursive equation:

$$
p = 0.7 \times 1 + 0.1 \times 0 + 0.2 \times p \quad \Rightarrow \quad p = \frac{7}{8} = 0.875
$$

Similarly, the probabiliy of success in the transition from $T$ to $TL$ is also $\frac{7}{8}$. The probability of both transitions succeeding assuming independence is thus $\dfrac{49}{64} \approx 0.7656$


#### For `policy_simultaneous`

The expected number of transitions to go from $S_0$ to $TL$, conditioning on eventual success, is given by

$$
\begin{aligned}
\mathbb{E}(S_0 \rightarrow S_0) = 1 &+ \mathbb{P}(S_0 \rightarrow S_0)\,\mathbb{E}(X_{S_0 \rightarrow TL}) + \mathbb{P}(S_0 \rightarrow T)\,\mathbb{E}(X_{T \rightarrow TL}) \\
 &+ \mathbb{P}(S_0 \rightarrow L)\,\mathbb{E}(X_{L \rightarrow TL}) + \mathbb{P}(S_0 \rightarrow TL)\cdot 0.
\end{aligned}
$$

Substituting

$$
\begin{aligned}
\mathbb{P}(S_0 \rightarrow S_0) &= \frac{4}{95} &
\mathbb{P}(S_0 \rightarrow T) &= \frac{21}{95} \\
\mathbb{P}(S_0 \rightarrow L) &= \frac{21}{95} &
\mathbb{P}(S_0 \rightarrow TL) &= \frac{49}{95} \\
\mathbb{E}(X_{T \rightarrow TL}) &= \frac{9}{7} &
\mathbb{E}(X_{L \rightarrow TL}) &= \frac{9}{7}
\end{aligned}
$$

and solving for $\mathbb{E}(S_0 \rightarrow S_0)$ yields $\mathbb{E}(S_0 \rightarrow S_0) = \dfrac{149}{91} \approx 1.637$.

Incrementing by $1$ by virtue of the actuation step, the expected number of steps till success is approximately $2.637$.

Like in the previous scenario, we let $p$ be the probability of eventual success, we observe

$$
p = 0.49 \times 1 + 0.21 \times 0.875 + 0.21 \times 0.875 + 0.04 \times p \quad \Rightarrow \quad p = \dfrac{0.8575}{0.96} \approx 0.8932
$$

**Conclusion:**

`policy_simultaneous` achieves both shorter interactions (~$2.6$ vs ~$3.6$ steps) AND higher success rate (~$89\%$ vs ~$77\%$) with our assumed probabilities. However, `policy_simultaneous` could have a lower success rate if the abandonment probability from `ASK_BOTH` is significantly higher than from individual questions.

:::

## Q4. Implementation and Simulation

Following the sample code from the lecture notes ([cs702-ci/07_mdp/lecture at main · SMU-HCI/cs702-ci](https://github.com/SMU-HCI/cs702-ci/tree/main/07_mdp/lecture)), implement the smart home agent MDP based on Q1 and Q2. Use the policies from Q3, adjusting them as needed.

Run simulations and analyze the results. Simulate 1000 episodes for each policy. For each policy, compute:

- Success rate (percentage of episodes reaching the success state)
- Average episode length (number of interaction turns)
- Average cumulative reward per episode

Based on the results, discuss which policy performs better overall and explain why, given your transition probabilities.

::: {.callout-tip title="Answer" icon="false"}
### Implementation

The MDP implementation defines the state space, action space, transition model, and reward function from Q1. The simulation code runs episodes following each policy until reaching a terminal state.

```{python}
#| label: mdp-simulation
import random
from enum import Enum
from typing import Dict, List, Tuple

class State(Enum):
    """State space for the smart home agent MDP."""
    S0 = "S0"       # Initial state - no preferences collected
    T = "T"         # Temperature preference collected
    L = "L"         # Lighting preference collected
    TL = "TL"       # Both preferences collected (ready to actuate)
    SUCCESS = "SUCCESS"  # Terminal - successful interaction
    FAILURE = "FAILURE"  # Terminal - user abandoned

class Action(Enum):
    """Action space for the smart home agent MDP."""
    ASK_TEMPERATURE = "ASK_TEMPERATURE"
    ASK_LIGHTING = "ASK_LIGHTING"
    ASK_BOTH = "ASK_BOTH"
    ACTUATE = "ACTUATE"

# Transition model: (state, action) -> [(next_state, probability), ...]
TRANSITIONS: Dict[Tuple[State, Action], List[Tuple[State, float]]] = {
    # From S0
    (State.S0, Action.ASK_TEMPERATURE): [
        (State.T, 0.7),      # Clear response
        (State.S0, 0.2),     # Unclear response
        (State.FAILURE, 0.1) # Abandon
    ],
    (State.S0, Action.ASK_LIGHTING): [
        (State.L, 0.7),
        (State.S0, 0.2),
        (State.FAILURE, 0.1)
    ],
    (State.S0, Action.ASK_BOTH): [
        (State.TL, 0.49),    # Both clear
        (State.T, 0.21),     # Temperature clear, lighting unclear
        (State.L, 0.21),     # Lighting clear, temperature unclear
        (State.S0, 0.04),    # Both unclear
        (State.FAILURE, 0.05) # At least one abandonment
    ],
    # From T (temperature collected, ask lighting)
    (State.T, Action.ASK_LIGHTING): [
        (State.TL, 0.7),
        (State.T, 0.2),
        (State.FAILURE, 0.1)
    ],
    # From L (lighting collected, ask temperature)
    (State.L, Action.ASK_TEMPERATURE): [
        (State.TL, 0.7),
        (State.L, 0.2),
        (State.FAILURE, 0.1)
    ],
    # From TL (both collected, actuate)
    (State.TL, Action.ACTUATE): [
        (State.SUCCESS, 1)
    ]
}

# Reward values
REWARD_SUCCESS = 10
REWARD_FAILURE = 0

def mdp_step(state: State, action: Action) -> Tuple[State, float]:
    """Execute one step of the MDP."""
    if state in (State.SUCCESS, State.FAILURE):
        return state, 0

    transitions = TRANSITIONS.get((state, action))
    if transitions is None:
        raise ValueError(f"No transition defined for state {state} and action {action}")

    r = random.random()
    cumulative = 0.0
    for next_state, prob in transitions:
        cumulative += prob
        if r <= cumulative:
            break

    reward = REWARD_SUCCESS if next_state == State.SUCCESS else REWARD_FAILURE
    return next_state, reward

def policy_sequential(state: State) -> Action:
    """Ask questions one at a time: temperature first, then lighting."""
    if state == State.S0:
        return Action.ASK_TEMPERATURE
    elif state == State.T:
        return Action.ASK_LIGHTING
    elif state == State.L:
        return Action.ASK_TEMPERATURE
    elif state == State.TL:
        return Action.ACTUATE
    else:
        raise ValueError(f"No action for state {state}")

def policy_simultaneous(state: State) -> Action:
    """Ask both questions together when possible."""
    if state == State.S0:
        return Action.ASK_BOTH
    elif state == State.T:
        return Action.ASK_LIGHTING
    elif state == State.L:
        return Action.ASK_TEMPERATURE
    elif state == State.TL:
        return Action.ACTUATE
    else:
        raise ValueError(f"No action for state {state}")

def sample_episode(policy):
    """Run a single episode until terminal state."""
    state = State.S0
    states_visited = [state]
    cumulative_reward = 0

    while state not in (State.SUCCESS, State.FAILURE):
        action = policy(state)
        next_state, reward = mdp_step(state, action)
        cumulative_reward += reward
        state = next_state
        states_visited.append(state)

    return states_visited, cumulative_reward

def run_simulation(policy, num_episodes=1000):
    """Run multiple episodes with a given policy."""
    successes = 0
    total_length = 0
    total_reward = 0

    for _ in range(num_episodes):
        states, reward = sample_episode(policy)
        episode_length = len(states) - 1

        if states[-1] == State.SUCCESS:
            successes += 1
            total_length += episode_length

        total_reward += reward

    return {
        "success_rate": successes / num_episodes,
        "avg_episode_length_given_success": total_length / successes
    }

# Run simulations with more episodes for stable results
num_episodes = 100_000
seq_results = run_simulation(policy_sequential, num_episodes)
sim_results = run_simulation(policy_simultaneous, num_episodes)
```

### Results

```{python}
#| label: mdp-results
print("=" * 60)
print(f"SIMULATION RESULTS ({num_episodes} episodes each)")
print("=" * 60)
print(f"\n{'Metric':<30} {'Sequential':>14} {'Simultaneous':>14}")
print("-" * 60)
print(f"{'Success Rate':<30} {seq_results['success_rate']*100:>13.1f}% {sim_results['success_rate']*100:>13.1f}%")
print(f"{'Avg Episode Length':<30} {seq_results['avg_episode_length_given_success']:>14.2f} {sim_results['avg_episode_length_given_success']:>14.2f}")
print("-" * 60)
print("\nTheoretical values (from Q3 analysis):")
print("  Sequential: success ~76.6%, avg length ~3.57")
print("  Simultaneous: success ~89.3%, avg length ~2.64")
```

:::
