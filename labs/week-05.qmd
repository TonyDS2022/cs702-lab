---
title: "Lab Week 5. Markov Decision Process (MDP)"
mermaid-format: png
---

## Introduction

Read the scenario below and complete Q1–Q4. Each question builds on the previous one, progressing from model specification to policy analysis to implementation.

A smart home conversational agent helps users adjust their environment by gathering preferences. The agent must collect temperature preference (warm/cool) and lighting preference (bright/dim) before making adjustments. Users may respond clearly, give unclear responses, or abandon the interaction if frustrated. The agent must decide: should it ask questions one at a time or both at once?

## Q1. MDP Formulation

Model the smart home agent as an MDP by specifying:

- **State space**: Define the initial state, interim states (where the agent has partial information), and terminal states.

- **Action space**: For each non-terminal state, list the available actions.

- **Transition probabilities**: For state $s_0$ (initial state with no information) and action `ask_temperature`, specify the probability distribution over next states. Consider realistic cases where users might provide clear answers, unclear responses, or abandon.

- **Reward function**: Define the reward $R(s, a, s')$ for reaching terminal states.

Explain your design choices.

::: {.callout-tip title="Answer" icon=false}
### State Space

The state space consists of discrete states representing what information has been collected:

- **$S_0$**: Initial state - no preferences collected
- **$T$**: Temperature preference collected
- **$L$**: Lighting preference collected
- **$TL$**: Both preferences collected (ready to actuate)
- **SUCCESS**: Terminal state - successful interaction
- **FAILURE**: Terminal state - user abandoned

### Action Space

For each non-terminal state, available actions are:

| State | Available Actions |
|-------|-------------------|
| $S_0$ | ASK_TEMPERATURE, ASK_LIGHTING, ASK_BOTH |
| $T$ | ASK_LIGHTING |
| $L$ | ASK_TEMPERATURE |
| $TL$ | ACTUATE |

### Transition Probabilities

From the initial state $S_0$:

| Origin | Action | Destination | Probability Mass |
|--------|--------|-------------|------------------|
| $S_0$ | ASK_TEMPERATURE | $T$ | 0.7 |
| $S_0$ | ASK_TEMPERATURE | $S_0$ | 0.2 |
| $S_0$ | ASK_TEMPERATURE | FAILURE | 0.1 |
| $S_0$ | ASK_LIGHTING | $L$ | 0.7 |
| $S_0$ | ASK_LIGHTING | $S_0$ | 0.2 |
| $S_0$ | ASK_LIGHTING | FAILURE | 0.1 |
| $S_0$ | ASK_BOTH | $TL$ | 0.49 |
| $S_0$ | ASK_BOTH | $T$ | 0.21 |
| $S_0$ | ASK_BOTH | $L$ | 0.21 |
| $S_0$ | ASK_BOTH | $S_0$ | 0.04 |
| $S_0$ | ASK_BOTH | FAILURE | 0.05 |

From states $T$, $L$:

| Origin | Action | Destination | Probability Mass |
|--------|--------|-------------|------------------|
| $T$ | ASK_LIGHTING | $TL$ | 0.7 |
| $T$ | ASK_LIGHTING | $T$ | 0.2 |
| $T$ | ASK_LIGHTING | FAILURE | 0.1 |
| $L$ | ASK_TEMPERATURE | $TL$ | 0.7 |
| $L$ | ASK_TEMPERATURE | $L$ | 0.2 |
| $L$ | ASK_TEMPERATURE | FAILURE | 0.1 |

From state $TL$:

| Origin | Action | Destination | Probability Mass |
|--------|--------|-------------|------------------|
| $TL$ | ACTUATE | SUCCESS | 0.9 |
| $TL$ | ACTUATE | FAILURE | 0.1 |

### Reward Function

Using a discounting model with discount factor $\gamma = 0.95$:

| Transition | Reward |
|------------|--------|
| Any ask action | $-\gamma^t$ (step cost) |
| $\to$ SUCCESS | $+\gamma^t \times 10$ |
| $\to$ FAILURE | $0$ |

where $t$ is the current time step (0-indexed).

### Design Choices

**Discrete State Space**: The state space tracks only what information has been collected (temperature, lighting, both, or neither). This abstraction models user frustration with transition probability masses.

**Transition Probabilities**: Each question has three possible outcomes:
- Clear response (70%): Successfully collect the preference
- Unclear response (20%): User responds but incompletely; agent retries from the same state
- Abandon (10%): User gives up; terminal `FAILURE` state

**ASK_BOTH Compound Probabilities**: When asking both questions simultaneously, outcomes are independent. The probability of getting both clear answers is $0.7 \times 0.7 = 0.49$. Partial successes (one clear, one unclear) transition to states $T$ or $L$, allowing the agent to recover.

**Discounting Model**: Rather than a fixed penalty per question, rewards decay over time with $\gamma = 0.95$. This naturally encourages efficiency—earlier `SUCCESS`'s are worth more—while still allowing longer interactions if necessary. `FAILURE` receives $0$ reward, incentivizing the agent to maximize success probability rather than rush.
:::

## Q2. State Transition Diagram

Draw a state transition diagram for your MDP showing:

- All states (use circles or boxes)

- Possible transitions between states for at least one action from the initial state

- Transition probabilities on the edges

Your diagram should help visualize how the agent and user move through the interaction.

## Q3. Policy Analysis

Consider the following Python code that implements two policies for the smart home agent:

```python
def policy_sequential(state: State) -> Action:
    """Ask questions one at a time: temperature first, then lighting."""
    if state == State.S0:
        return Action.ASK_TEMPERATURE
    elif state == State.T:
        return Action.ASK_LIGHTING
    elif state == State.L:
        return Action.ASK_TEMPERATURE
    elif state == State.TL:
        return Action.ADJUST
    else:
        raise ValueError(f"No action for state {state}")


def policy_simultaneous(state: State) -> Action:
    """Ask both questions together when possible."""
    if state == State.S0:
        return Action.ASK_BOTH
    elif state == State.T:
        return Action.ASK_LIGHTING
    elif state == State.L:
        return Action.ASK_TEMPERATURE
    elif state == State.TL:
        return Action.ADJUST
    else:
        raise ValueError(f"No action for state {state}")
```

Explain the key difference between these two policies. Which policy would likely achieve a shorter average interaction length? Why? Under what conditions (consider transition probabilities) might `policy_simultaneous` have a lower success rate than `policy_sequential`?

## Q4. Implementation and Simulation

Following the sample code from the lecture notes ([cs702-ci/07_mdp/lecture at main · SMU-HCI/cs702-ci](https://github.com/SMU-HCI/cs702-ci/tree/main/07_mdp/lecture)), implement the smart home agent MDP based on Q1 and Q2. Use the policies from Q3, adjusting them as needed.

Run simulations and analyze the results. Simulate 1000 episodes for each policy. For each policy, compute:

- Success rate (percentage of episodes reaching the success state)
- Average episode length (number of interaction turns)
- Average cumulative reward per episode

Based on the results, discuss which policy performs better overall and explain why, given your transition probabilities.

## Deliverable

Submit a zip file to eLearn containing:

1. Written answers to Q1–Q4 with explanations and diagrams.

2. Python code for Q4 implementing the MDP and policy simulations.
