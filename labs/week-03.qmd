---
title: "Lab Week 3. Bayesian Inference"
mermaid-format: png
---

```{python}
#| echo: false
#| include: false

import numpy as np
```

## Introduction

This week's lab explores Bayesian inference—a framework for updating beliefs based on new evidence. You'll learn to formulate probabilistic models, represent them graphically using plate notation, and implement them with NumPyro, a probabilistic programming language built on JAX.

Read the scenario below and work through Q1–Q4. Each question builds on the previous one, moving from model specification to implementation and inference.

> An e-commerce company has collected user log data showing how users interact with their website to complete purchase transactions. Each transaction involves several steps, such as product selection, shipping information entry, and payment processing. The company wants to use this transaction log data to model user behavior using Bayesian inference.

## Q1. Probabilistic Modeling

Assume the total time users spend completing the checkout process follows an exponential distribution with rate parameter $\lambda$. Given that the average purchase time is approximately 10 seconds, build a Bayesian model by setting a prior over $\lambda$ rather than directly fixing $\lambda = 0.1$. Propose a reasonable prior distribution for $\lambda$, specify its parameters, and briefly explain your choice. Then, express the complete probabilistic model mathematically.

::: {.callout-tip title="Answer" icon=false}

We desire a distribution to encode our prior belief about the rate parameter $\lambda$ should approximately be $0.1$. Importantly, the distribution should satisfy the property $\lambda > 0$ and be a conjugate prior to the exponential distribution.

We can select $\lambda \sim \Gamma(1, 10)$ as a weakly informative prior with mean $0.1$ and standard deviation $0.1$.

The complete probabilistic model is given by:

$$
\begin{aligned}
\lambda_{0} &\sim \Gamma(1, 10), \\
\lambda | T_{1:n} &\sim \Gamma\left(1 + n, 10 + \sum_{i=1}^{n} T_i\right), \\
T_{i} | \lambda &\sim \text{Exp}(\lambda),\;i=1,\,\ldots,\,n.
\end{aligned}
$$

:::

## Q2. Plate Diagram

Draw the plate notation for your model. Recall that a plate diagram constitutes a graphical representation of a probabilistic model where nodes represent random variables, edges represent dependencies, and plates (rectangles) represent repeated sampling or independent replications of variables across multiple observations or groups.

::: {.callout-tip title="Answer" icon=false}

The plate diagram for the model in Q1 is shown below. The rate parameter $\lambda$ is sampled once from the Gamma prior, while the checkout times $T_i$ are conditionally independent given $\lambda$ and sampled repeatedly for each of the $n$ observations. The shaded node indicates that $T_i$ is observed.

::: {.content-visible when-format="html"}
![](figures/plate-diagram-q1.svg){fig-align="center" width="40%"}
:::

::: {.content-visible when-format="pdf"}
![](figures/plate-diagram-q1-light.png){fig-align="center" width="40%"}
:::

**Legend:**

- $\lambda \sim \Gamma(1, 10)$ — latent rate parameter (unshaded)
- $T_i | \lambda \sim \text{Exp}(\lambda)$ — observed checkout times (shaded)
- The plate indicates $i = 1, \ldots, n$ independent observations

:::

## Q3. Probabilistic Programming Language (PPL)

Consider the following code that defines a hierarchical probabilistic model using the NumPyro library:

```python
def hierarchical_model(group_ids=None, checkout_times=None):
    """
    Args:
        group_ids: Group IDs for each observation (0=Group A, 1=Group B)
        checkout_times: Observed checkout times
    """
    alpha_groups = jnp.array([12.0, 8.0])
    beta_groups = jnp.array([100.0, 100.0])

    lambda_groups = numpyro.sample(
        "lambda_groups", dist.Gamma(alpha_groups, beta_groups).expand([2])
    )

    with numpyro.plate(
        "data", len(checkout_times) if checkout_times is not None else 1
    ):
        numpyro.sample(
            "checkout_times",
            dist.Exponential(lambda_groups[group_ids]),
            obs=checkout_times,
        )
```

Describe the model's structure and explain how it differs from the models in Q1 and Q2. Interpret the prior distributions for each group and what they suggest about expected checkout times. Then, explain what this model means for the e-commerce company and what insights might it provide.

::: {.callout-tip title="Answer" icon=false}

The code describes a hierarchical Bayesian model with two user groups (A and B), parameterized by rate parameters $\lambda_A$ and $\lambda_B$. Formally:

$$
\begin{aligned}
\lambda_A &\sim \Gamma(12, 100), \\
\lambda_B &\sim \Gamma(8, 100), \\
T_i | \lambda_{g_i} &\sim \text{Exp}(\lambda_{g_i}), \quad i = 1, \ldots, n,
\end{aligned}
$$

where $g_i \in \{A, B\}$ is the group assignment for observation $i$.

The model described in Q1 and Q2 is parameterized by a single rate parameter $\lambda$ across all observations, implyling that user behavior is homogeneous across all groups. The model in this question allows heterogeneous user behavior based on user segment membership.

:::

## Q4. Using PPL

Write a Python function using NumPyro to implement the model from Q1. Your function should:

- Define a prior distribution for the rate parameter $\lambda$ based on your answer to Q1
- Use an Exponential likelihood for the observed checkout times
- Accept observed checkout times as input

Then, use your function to perform inference on the following synthetic dataset of checkout times (in seconds):

```{python}
#| eval: true
#| output: false

import jax.numpy as jnp
checkout_times = jnp.array([8.2, 12.5, 9.1, 15.3, 7.8, 11.2, 10.5, 13.7, 9.9, 14.1])
```

Use MCMC with the NUTS sampler to obtain posterior samples. Report:

1. The posterior mean and 95% highest density interval for $\lambda$
2. A visualization of the prior and posterior distributions for $\lambda$
3. Based on your posterior inference, what is the updated expected checkout time?

::: {.callout-tip title="Answer" icon=false}

Before we implement the `numpyro` based solutions, we observe that the posterior distributions has closed form analytical solutions.

Recall that we have a weakly informative prior in the form $\lambda \sim \Gamma(\alpha,\,\beta)$ and the checkout times $T_{i}|\lambda \sim \text{Exp}(\lambda)$.

From Bayes', we have the predictive posterior density

$$
\begin{align}
p(T_{n+1}=x\,|\,\mathbf{T}) &= \int_{0}^{\infty} p(T_{n+1} = x\,|\,\lambda) \, p(\lambda\,|\,\mathbf{T})\,d\lambda \\
&= \int_{0}^{\infty} \lambda e^{-\lambda x} \frac{\beta^{\alpha}}{\Gamma(\alpha)} \lambda^{\alpha - 1} e^{-\beta \lambda}\,d\lambda \\
&= \frac{\beta^{\alpha}}{\Gamma(\alpha)} \int_{0}^{\infty} \lambda^{\alpha} e^{-(\beta + x)\lambda}\,d\lambda
\end{align}
$$

We recall the Gamma identify $\Gamma(n) = \displaystyle\int_{0}^{\infty} t^{n - 1}e^{-t}\,dt$, thus

$$
\int_{0}^{\infty} \lambda^{\alpha} e^{-(\beta + x)\lambda}\,d\lambda = \frac{\Gamma(\alpha + 1)}{(\beta + x)^{\alpha + 1}} = \frac{\alpha!}{(\beta + x)^{\alpha + 1}}.
$$

The predictive posterior density therefore reduces to
$$
p(T_{n+1}=x\,|\,\mathbf{T}) = \frac{\alpha \beta^{\alpha}}{(\beta + x)^{\alpha + 1}},\;\text{where } x \geq 0.
$$

This is a Pareto type II distribution with shape $\alpha$ and scale $\beta$. The first and second central moments have closed forms

$$
\mathbb{E}(T|\mathbf{T}) = \frac{\beta}{\alpha - 1},\;\text{and}
$$

$$
\text{Var}(T|\mathbf{T}) = \frac{\beta^{2}\alpha}{(\alpha - 1)^{2}(\alpha - 2)}
$$

At $\alpha = 11$ and $\beta = 122.3$, the posterior mean evaluates to `{python} f"{122.3/(11 - 1):.2f}"` seconds and the posterior standard deviation evaluates to `{python} f"{np.sqrt(122.3**2 * 11 / ((11 - 1)**2 * (11 - 2))):.2f}"` seconds.

We can now verify this with a `numpyro` implementation.
```{python}
#| eval: true
#| output: true

import jax
import jax.numpy as jnp
import numpy as np
import numpyro
import numpyro.distributions as dist
from numpyro.infer import MCMC, NUTS
from jax.scipy.stats import gamma as gamma_dist
import plotly.graph_objects as go

def checkout_model(checkout_times=None):
    lam = numpyro.sample("lambda", dist.Gamma(1.0, 10.0))
    with numpyro.plate("data", len(checkout_times) if checkout_times is not None else 1):
        numpyro.sample("obs", dist.Exponential(lam), obs=checkout_times)

kernel = NUTS(checkout_model)
mcmc = MCMC(kernel, num_warmup=1000, num_samples=5000, num_chains=1)
mcmc.run(jax.random.PRNGKey(42), checkout_times=checkout_times)
mcmc.print_summary()
```

```{python}
#| eval: true
#| output: true

posterior_samples = mcmc.get_samples()["lambda"]
posterior_mean = float(jnp.mean(posterior_samples))
sorted_samples = jnp.sort(posterior_samples)
hdi_lower = float(sorted_samples[int(0.025 * len(sorted_samples))])
hdi_upper = float(sorted_samples[int(0.975 * len(sorted_samples))])

print(f"Posterior mean of λ: {posterior_mean:.4f}")
print(f"95% HDI: [{hdi_lower:.4f}, {hdi_upper:.4f}]")
print(f"Updated expected checkout time (1/λ): {1/posterior_mean:.2f} seconds")
```

```{python}
#| eval: true
#| output: true
#| label: fig-prior-posterior
#| fig-cap: "Prior and posterior distributions for λ"

import matplotlib.pyplot as plt

x = np.linspace(0.01, 0.3, 500)
prior_pdf = np.exp(gamma_dist.logpdf(jnp.array(x), 1.0, scale=1/10.0))

hist_counts, bin_edges = np.histogram(
    np.array(posterior_samples),
    bins=50,
    density=True,
)
bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2

is_html = True
try:
    import plotly.graph_objects as go
except ImportError:
    is_html = False
```

::: {.content-visible when-format="html"}
```{python}
#| eval: true
#| output: true

fig = go.Figure()
fig.add_trace(go.Bar(
    x=bin_centers, y=hist_counts, width=bin_edges[1] - bin_edges[0],
    name="Posterior samples",
    marker_color="rgba(253, 115, 10, 0.6)",
    marker_line_color="#FD730A",
    marker_line_width=1,
))
fig.add_trace(go.Scatter(
    x=x, y=np.array(prior_pdf),
    mode="lines", name="Prior: Γ(1, 10)",
    line=dict(color="#248DCC", width=2, dash="dash"),
))
fig.add_vline(
    x=posterior_mean, line_dash="dot", line_color="#67A902",
    annotation_text=f"Posterior mean = {posterior_mean:.4f}",
    annotation_font_color="#67A902",
)
fig.update_layout(
    template="plotly_dark",
    paper_bgcolor="rgba(0,0,0,0)",
    plot_bgcolor="rgba(0,0,0,0)",
    xaxis_title="λ",
    yaxis_title="Density",
    title="Prior vs Posterior for λ",
    font=dict(color="#fff"),
    legend=dict(font=dict(color="#fff")),
)
fig.show()
```
:::

::: {.content-visible when-format="pdf"}
```{python}
#| eval: true
#| output: true

import seaborn as sns

sns.set_theme(style="whitegrid", font_scale=1.1, rc={
    "axes.edgecolor": "#444444",
    "grid.color": "#e0e0e0",
    "grid.linestyle": "--",
    "grid.alpha": 0.4,
    "figure.facecolor": "white",
})

fig, ax = plt.subplots(figsize=(8, 4.5))

ax.fill_between(
    x, np.array(prior_pdf), alpha=0.15, color="#248DCC",
)
ax.plot(x, np.array(prior_pdf), color="#248DCC", linewidth=2.5,
        linestyle="--", label=r"Prior: $\Gamma(1, 10)$")

sns.histplot(
    np.array(posterior_samples), bins=50, stat="density",
    color="#FD730A", alpha=0.55, edgecolor="white", linewidth=0.5,
    label="Posterior samples", ax=ax,
)

ax.axvline(posterior_mean, linestyle=":", color="#67A902", linewidth=2.5,
           label=f"Posterior mean = {posterior_mean:.4f}")
ax.axvspan(hdi_lower, hdi_upper, alpha=0.08, color="#67A902",
           label=f"95% HDI [{hdi_lower:.4f}, {hdi_upper:.4f}]")

ax.set_xlabel(r"$\lambda$ (rate parameter)", fontsize=12)
ax.set_ylabel("Density", fontsize=12)
ax.set_title(
    r"Prior vs Posterior Distribution for $\lambda$",
    fontsize=14, fontweight="bold",
    color="#67A902",
)
ax.legend(frameon=True, fancybox=True, shadow=True, fontsize=10)
sns.despine(left=True)
plt.tight_layout()
plt.show()
```
:::

:::
